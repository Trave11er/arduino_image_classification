{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_face_dataset = angry + disgust combined into frown\n",
    "data_dir = Path('/home/gleb/programming/play/tiny_ml/mini_face_dataset')\n",
    "batch_size = 32\n",
    "img_height = 48\n",
    "img_width = 48\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_dir / 'train'),\n",
    "  seed=0,\n",
    "  color_mode='grayscale',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_dir / 'validation'),\n",
    "  seed=0,\n",
    "  color_mode='grayscale',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# Cast data to float\n",
    "train_ds.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
    "val_ds.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "dict_ = {}\n",
    "for i in range(num_classes):\n",
    "    dict_[i] = class_names[i]\n",
    "\n",
    "model_path = './saved_model_weights_v3/best_model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  1)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model from tf tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/images/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 1)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=15\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model from GSoC post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/theevildoof/TinyML_OV7670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import DepthwiseConv2D\n",
    "from keras.backend import relu\n",
    "from keras.activations import softmax\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(DepthwiseConv2D((3,3),input_shape=(img_height,img_width,1)))\n",
    "# Convolution Layer 1\n",
    "model.add(Conv2D(2, (3, 3)))      # 2 different 3x3 kernels -- so 2 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "convLayer01 = Activation('relu')                     # activation\n",
    "model.add(convLayer01)\n",
    "\n",
    "# Convolution Layer 2\n",
    "model.add(Conv2D(2, (3, 3)))                        # 2 different 3x3 kernels -- so 2 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "model.add(Activation('relu'))                        # activation\n",
    "convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n",
    "model.add(convLayer02)\n",
    "\n",
    "# Convolution Layer 3\n",
    "model.add(Conv2D(4,(3, 3)))                         # 4 different 3x3 kernels -- so 4 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "convLayer03 = Activation('relu')                     # activation\n",
    "model.add(convLayer03)\n",
    "\n",
    "# Convolution Layer 4\n",
    "model.add(Conv2D(4, (3, 3)))                        # 4 different 3x3 kernels -- so 64 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "model.add(Activation('relu'))                        # activation\n",
    "convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n",
    "layers.Dropout(0.2)\n",
    "model.add(convLayer04)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5,activation = relu))\n",
    "model.add(Dense(num_classes, activation = softmax))\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=15\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model v3 (ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, AvgPool2D\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers.convolutional import DepthwiseConv2D\n",
    "from keras.backend import relu\n",
    "from keras.activations import softmax\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(DepthwiseConv2D((3,3),input_shape=(img_width,img_height,1)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(2, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "convLayer01 = Activation('relu')\n",
    "\n",
    "model.add(Conv2D(2, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(convLayer01)\n",
    "convLayer015 = AvgPool2D(pool_size=(2,2))\n",
    "model.add(convLayer015)\n",
    "\n",
    "model.add(Conv2D(4, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(4, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "convLayer02 = AvgPool2D(pool_size=(2,2))\n",
    "model.add(convLayer02)\n",
    "\n",
    "model.add(Conv2D(8,(3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "convLayer03 = Activation('relu')\n",
    "\n",
    "model.add(Conv2D(8,(3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "convLayer03 = Activation('relu')\n",
    "convLayer04 = AvgPool2D(pool_size=(2,2))\n",
    "model.add(convLayer04)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation = relu, kernel_regularizer=regularizers.L2(0.05)))\n",
    "model.add(Dense(num_classes, activation = softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.build((None, img_height, img_height, 1))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 45\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  predictions = model.predict(images)\n",
    "  max_predictions = np.argmax(predictions, axis=1)\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(f'{dict_[max_predictions[i]]} ({class_names[labels[i]]})')\n",
    "    plt.axis(\"off\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b470ca90f7e831715a5a891c24658ec902099d8cd31269e998c1bab8015d49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
